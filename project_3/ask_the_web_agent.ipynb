{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‚Äëthe‚ÄëWeb Agent**\n",
    "\n",
    "Welcome to Project‚ÄØ3! In this project, you will learn how to use tool‚Äëcalling LLMs, extend them with custom tools, and build a simplified *Perplexity‚Äëstyle* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‚Äëcalling capability\n",
    "* Combine LLM with a web‚Äësearch tool to build a simple ask‚Äëthe‚Äëweb agent.\n",
    "* Connect to external tools using **MCP (Model Context Protocol)**, a universal standard for LLM‚Äëtool integration.\n",
    "* Optionally build a UI using Chainlit to test your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "0. Environment setup\n",
    "1. Write simple tools and connect them to an LLM\n",
    "2. Standardize tool calling with JSON schemas\n",
    "3. Use LangGraph for tool calling\n",
    "4. Build a Perplexity-style web-search agent\n",
    "5. (Optional) MCP: connect to external tool servers\n",
    "6. (Optional) A minimal UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 0- Environment setup\n",
    "\n",
    "### Step 1: Create your environment and install dependencies \n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook, and use Conda or uv to install the project dependencies.\n",
    "\n",
    "#### Option 1: Conda\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "```\n",
    "\n",
    "#### Option 2: UV (faster)\n",
    "\n",
    "If you prefer [uv](https://docs.astral.sh/uv/) over Conda:\n",
    "\n",
    "```bash\n",
    "# Install uv (skip if already installed)\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Create venv and install dependencies\n",
    "uv venv .venv-web-agent-uv && source .venv-web-agent-uv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 2: Register this environment as a Jupyter kernel\n",
    "```bash\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Now open your notebook and switch to the `web_agent` kernel (Kernel ‚Üí Change Kernel).\n",
    "\n",
    "### Step 3: Set up Ollama\n",
    "\n",
    "In this project, we use **Ollama** to load and use open-weight LLMs. We start with smaller models like `gemma3:1b` and then switch to larger models like `llama3.2:3b`.\n",
    "\n",
    "Start the **Ollama** server in a terminal. This launches a local API endpoint that listens for LLM requests.\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Downloads the model so you can run them locally without API calls. \n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "You can explore other available models [here](https://ollama.com/library) and pull them to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bejkbbzrdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: is Ollama running?\n",
    "# If this fails, open a terminal and run: ollama serve\n",
    "\n",
    "import httpx\n",
    "\n",
    "response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Ollama is running. Installed models: {models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 1- Tool¬†Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "<img src=\"assets/tools.png\" width=\"700\">\n",
    "\n",
    "As show below, We first implement a tool, then describe the tool as part of the model's prompt. When the model decides that a tool is needed, it emits a structured output. A parser will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "<img src=\"assets/tool_flow.png\" width=\"700\">\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3:1b` model to use it when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23¬∞C and sunny in San Francisco.\")\n",
    "#\n",
    "# Output:\n",
    "#   ‚Ä¢ Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") ‚Üí \"It is 23¬∞C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Step 2: Create a prompt to teach the LLM when and how to use your tool\n",
    "# ----------------------------------------------------------------------\n",
    "# What to include:\n",
    "#   ‚Ä¢ A SYSTEM_PROMPT that tells the model about the tool use and describes the tool\n",
    "#   ‚Ä¢ A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Create an Ollama client\n",
    "#   2. Use chat.completions.create to send your prompt to gemma3:1b\n",
    "#   3. Print the response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "from openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Manually parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23¬∞C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~7-10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 2- Standardize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing a function, manually teaching the LLM about it, and write a regex to parse the output. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function's signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Rewrite the get_current_weather function with docstring and arg types\n",
    "#   2. Use `inspect.signature` to automatically get function parameters and docstring\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing: name, description, and parameters.\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~2-3 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "def to_schema(fn):\n",
    "    sig = inspect.signature(fn)\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model instead of prompt surgery\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user‚Äôs question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., gemma3:1b).\n",
    "#   4. Print the model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 3- LangChain for Tool Calling\n",
    "\n",
    "So far, you built a simple tool-calling pipeline. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning. We have to write manual parsers, function calling logic, and adding responses back to the prompt.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward. In this section, you will create a **ReAct** Agent (Reasoning + Acting). As shown below, the model alternates between reasoning steps and tool use wihtout any manual work.\n",
    "\n",
    "<img src=\"assets/react.png\" width=\"500\">\n",
    "\n",
    "The following links might be helpful for completing this section:\n",
    "- [Create Agents](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "- [LangChain Tools](https://docs.langchain.com/oss/python/langchain/tools)\n",
    "- [Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Keep your existing `get_current_weather` function as before.\n",
    "#   2. Create a new function (e.g., get_weather) that calls it.\n",
    "#   3. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   ‚Ä¢ The decorator converts your Python function into a standardized tool object.\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa159c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the Agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a gemma3:1b LLM instance \n",
    "#   2. Create the agent using create_agent\n",
    "#   3. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~6 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5824437",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "Your run failed because `gemma3:1b` does not support native tool calling (function calling). LangChain expects the model to return a structured tool-call object, but `gemma3:1b` can only return plain text, so the tool invocation step breaks.\n",
    "\n",
    "### Why previosuly, our manual approach worked with any model?\n",
    "\n",
    "In previous sections, we used **text-based tool calling**. We described the tool format in the system prompt. We asked the model to output `TOOL_CALL: {\"name\": ..., \"args\": ...}`. We then parsed this text with regex.\n",
    "\n",
    "This works with **any model** (even small ones like `gemma3:1b`) because we're just asking the model to follow a certain structured output format.\n",
    "\n",
    "### Why LangChain requires specific models?\n",
    "\n",
    "LangChain relies on **native tool calling** and it expects a consistent structured output format irrespective of the model. Hence, it enfornces model outputs structured tool calls in a specific format. This requires models trained specifically for function calling\n",
    "\n",
    "**Rule of thumb**: Models under 3B parameters typically lack native tool-calling capability.\n",
    "\n",
    "| Model | Size | Native Tool Support | Notes |\n",
    "|-------|------|---------------------|-------|\n",
    "| `gemma3:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:1b` | 1B | No | Works for manual approach only |\n",
    "| `llama3.2:3b` | 3B | Yes | Good balance of speed and capability |\n",
    "| `gemma3` | 4B | Yes | Supports native tools |\n",
    "| `mistral` | 7B | Yes | Strong tool support |\n",
    "\n",
    "Let's fix the issue we observed in the previous cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2 (retry): Re-create the Agent with a native tool-calling LLM\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create a llama3.2:3b LLM instance \n",
    "#   2. Create a system prompt to teach react-style reasoning to the LLM\n",
    "#   3. Create the agent using create_agent\n",
    "#   4. Test the agent with a natural question using agent.invoke\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yn0fe4dnbf",
   "metadata": {},
   "source": [
    "## 4- Web Search Agent\n",
    "\n",
    "Now that you know how to use LangChain with tools, let's build something useful. Instead of a toy get_weather tool, let create an agent that searches the web and answers questions using real results. In the next section, you will create a [DuckDuckGo](https://github.com/deedy5/ddgs) search tool and wire it into a ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Write a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Write a helper function (e.g., search_web) that:\n",
    "#        ‚Ä¢ Takes a query string\n",
    "#        ‚Ä¢ Uses DuckDuckGo (DDGS) to fetch top results (titles + URLs)\n",
    "#        ‚Ä¢ Returns them as a formatted string\n",
    "#   2. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~7 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Steps:\n",
    "#   1. Create an LLM (e.g., ChatOllama).\n",
    "#   2. Add your `web_search` tool to the tools list.\n",
    "#   3. Create the agent using create_agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent using agent.invoke\n",
    "# ---------------------------------------------------------\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyzwk2etyt",
   "metadata": {},
   "source": [
    "## 5- (Optional) MCP: Model Context Protocol\n",
    "\n",
    "Up to now, every tool you used started as a Python function you wrote and registered yourself. **MCP (Model Context Protocol)** lets you skip that step. Tools come from an external *server*, and your code just connects to it. Think of it like USB for AI tools: any MCP client can plug into any MCP server and immediately use whatever tools it offers.\n",
    "\n",
    "Below, we connect to `mcp-server-fetch` (a ready-made server that can retrieve any URL) using the Python MCP SDK. We launch the server, discover its tools, and call one, all without writing a single `@tool` function. To learn more, read: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "> **LangChain integration:** The `langchain-mcp-adapters` package can convert MCP tools into LangChain-compatible tools automatically, so you can drop them straight into a ReAct agent like the ones in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buxkz996bq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "# Create an MCP client session and connect it to mcp-server-fetch.\n",
    "# Follow this link: https://github.com/modelcontextprotocol/servers/tree/main/src/fetch\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7v9so18x",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load the tool using load_mcp_tools\n",
    "# create agent with llm and tools same as before\n",
    "# Fetch the content of a website like http://python.org\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6647eac",
   "metadata": {},
   "source": [
    "## 6- (Optional) A Minimal UI\n",
    "\n",
    "[Chainlit](https://chainlit.io/) is a Python library designed specifically for building LLM and agent UIs. It provides:\n",
    "- Built-in streaming support\n",
    "- Message history\n",
    "- Step visualization (see tool calls as they happen)\n",
    "- No frontend code required\n",
    "\n",
    "If you are interested, follow Chainlit's documentation to implement a simple UI for your agent. The process typically involves:\n",
    "\n",
    "1. You write a Python file named `chainlit_app.py` with the agent creation logic as well as UI handlers (e.g.,`@cl.on_message`)\n",
    "2. Run the file in your terminal with `chainlit run app.py`\n",
    "3. A web UI opens automatically at `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hi1y4z7r2y",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile chainlit_app.py\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit Web Search Agent\n",
    "\n",
    "import chainlit as cl\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from ddgs import DDGS\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the web search tool\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Create the agent (once at startup)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Chainlit message handler\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    \"\"\"Handle user messages and stream agent responses.\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Welcome message\n",
    "# ---------------------------------------------------------\n",
    "@cl.on_chat_start\n",
    "async def start(): \n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~5 lines of code)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116bbee",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have built a **web-enabled agent** from scratch: manual tool calling ‚Üí JSON schemas ‚Üí LangChain ReAct ‚Üí web search ‚Üí MCP ‚Üí UI.\n",
    "\n",
    "Next steps:\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "* Explore the [MCP server registry](https://github.com/modelcontextprotocol/servers) for ready-made tool servers.\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-web-agent-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
